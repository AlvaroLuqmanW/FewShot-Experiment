{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-24T03:25:33.702397Z",
     "iopub.status.busy": "2025-09-24T03:25:33.701954Z",
     "iopub.status.idle": "2025-09-24T03:25:33.710480Z",
     "shell.execute_reply": "2025-09-24T03:25:33.709515Z",
     "shell.execute_reply.started": "2025-09-24T03:25:33.702372Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/shared/huggingface\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"  # Make both GPUs visible\n",
    "os.environ[\"HF_HOME\"] = \"/home/shared/huggingface\"\n",
    "print(os.getenv(\"HF_HOME\"))\n",
    "# os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:true\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-24T03:25:33.712021Z",
     "iopub.status.busy": "2025-09-24T03:25:33.711785Z",
     "iopub.status.idle": "2025-09-24T03:25:37.431627Z",
     "shell.execute_reply": "2025-09-24T03:25:37.431012Z",
     "shell.execute_reply.started": "2025-09-24T03:25:33.712005Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())  # Should return True if GPU is available\n",
    "print(torch.cuda.device_count())  # Number of GPUs available\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-24T03:25:37.432606Z",
     "iopub.status.busy": "2025-09-24T03:25:37.432276Z",
     "iopub.status.idle": "2025-09-24T03:25:59.472854Z",
     "shell.execute_reply": "2025-09-24T03:25:59.472056Z",
     "shell.execute_reply.started": "2025-09-24T03:25:37.432579Z"
    },
    "id": "7mPHvz105dyD",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import random\n",
    "from typing import List, Dict, Tuple\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "import csv\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FEW-SHOT EXAMPLE BY EMBEDDING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-24T03:25:59.474991Z",
     "iopub.status.busy": "2025-09-24T03:25:59.474537Z",
     "iopub.status.idle": "2025-09-24T03:25:59.481063Z",
     "shell.execute_reply": "2025-09-24T03:25:59.480408Z",
     "shell.execute_reply.started": "2025-09-24T03:25:59.474972Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def read_jsonl(path):\n",
    "    arr = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for ln in f:\n",
    "            if ln.strip():\n",
    "                arr.append(json.loads(ln))\n",
    "    return arr\n",
    "\n",
    "def write_jsonl(objs, outpath):\n",
    "    with open(outpath, \"w\", encoding=\"utf-8\") as f:\n",
    "        for o in objs:\n",
    "            f.write(json.dumps(o, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "def read_csv_rows(path, text_col=\"preprocessed\", label_col=\"compact_label\", id_col=\"id\"):\n",
    "    rows = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for r in reader:\n",
    "            rows.append({\n",
    "                \"id\": r.get(id_col, \"\"),\n",
    "                \"text\": r.get(text_col, \"\").strip(),\n",
    "                \"label\": r.get(label_col, \"\").strip()\n",
    "            })\n",
    "    return rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GET_EXAMPLE_BY_EMBEDDING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-24T03:25:59.482087Z",
     "iopub.status.busy": "2025-09-24T03:25:59.481837Z",
     "iopub.status.idle": "2025-09-24T03:25:59.505336Z",
     "shell.execute_reply": "2025-09-24T03:25:59.504625Z",
     "shell.execute_reply.started": "2025-09-24T03:25:59.482063Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer, SimilarityFunction\n",
    "\n",
    "def get_example_bert_multilabel(test_file: str, \n",
    "                                 train_file: str, \n",
    "                                 num_of_example: int = 10, \n",
    "                                 output_file: str = \"test_with_example_bert.jsonl\",\n",
    "                                 bert_model: str = \"paraphrase-multilingual-MiniLM-L12-v2\",\n",
    "                                 sim_function: str = \"cosine\",\n",
    "                                 drop_duplicate_text: bool = True):\n",
    "    \"\"\"\n",
    "    Generate few-shot examples using BERT-based semantic similarity for multi-label classification.\n",
    "    \n",
    "    Args:\n",
    "        test_file: Path to test CSV file\n",
    "        train_file: Path to train CSV file\n",
    "        num_of_example: Number of examples to select for each label combination\n",
    "        output_file: Path to output JSONL file\n",
    "        bert_model: SentenceTransformer model name\n",
    "        sim_function: Similarity function ('cosine', 'dot', 'euclidean', 'manhattan')\n",
    "        drop_duplicate_text: Whether to drop duplicate texts from examples\n",
    "    \"\"\"\n",
    "    # Load the datasets\n",
    "    test_df = pd.read_csv(test_file)\n",
    "    train_df = pd.read_csv(train_file)\n",
    "\n",
    "    # Drop NaN in 'preprocessed' from train set only\n",
    "    train_df = train_df.dropna(subset=['preprocessed'])\n",
    "\n",
    "    # For test set, replace NaN in 'preprocessed' with 'text'\n",
    "    test_df['preprocessed'] = test_df.apply(\n",
    "        lambda row: row['text'] if pd.isna(row['preprocessed']) else row['preprocessed'],\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    # Define label mapping\n",
    "    label_mapping_hsal = {\n",
    "        (1, 1): \"HsAl\",\n",
    "        (1, 0): \"HsnAl\",\n",
    "        (0, 1): \"nHsAl\",\n",
    "        (0, 0): \"nHsnAl\"\n",
    "    }\n",
    "\n",
    "    # Filter train dataset based on labels\n",
    "    hsal_1_df = train_df[(train_df['final_label_hs'] == 1) & (train_df['final_label_al'] == 1)].reset_index(drop=True)\n",
    "    hsal_2_df = train_df[(train_df['final_label_hs'] == 1) & (train_df['final_label_al'] == 0)].reset_index(drop=True)\n",
    "    hsal_3_df = train_df[(train_df['final_label_hs'] == 0) & (train_df['final_label_al'] == 1)].reset_index(drop=True)\n",
    "    hsal_4_df = train_df[(train_df['final_label_hs'] == 0) & (train_df['final_label_al'] == 0)].reset_index(drop=True)\n",
    "\n",
    "    # Drop duplicates if requested\n",
    "    if drop_duplicate_text:\n",
    "        hsal_1_df = hsal_1_df.drop_duplicates(subset=['preprocessed'], keep='first').reset_index(drop=True)\n",
    "        hsal_2_df = hsal_2_df.drop_duplicates(subset=['preprocessed'], keep='first').reset_index(drop=True)\n",
    "        hsal_3_df = hsal_3_df.drop_duplicates(subset=['preprocessed'], keep='first').reset_index(drop=True)\n",
    "        hsal_4_df = hsal_4_df.drop_duplicates(subset=['preprocessed'], keep='first').reset_index(drop=True)\n",
    "\n",
    "    # Ensure there are enough samples\n",
    "    categories = {\n",
    "        'HsAl': hsal_1_df,\n",
    "        'HsnAl': hsal_2_df,\n",
    "        'nHsAl': hsal_3_df,\n",
    "        'nHsnAl': hsal_4_df\n",
    "    }\n",
    "    \n",
    "    for cat_name, cat_df in categories.items():\n",
    "        if len(cat_df) < num_of_example:\n",
    "            raise ValueError(f\"Not enough examples for {cat_name}: found {len(cat_df)}, need {num_of_example}\")\n",
    "\n",
    "    # Initialize model\n",
    "    print(f\"Loading BERT model: {bert_model}\")\n",
    "    if sim_function == \"cosine\":\n",
    "        model = SentenceTransformer(bert_model, similarity_fn_name=SimilarityFunction.COSINE)\n",
    "    elif sim_function == \"dot\":\n",
    "        model = SentenceTransformer(bert_model, similarity_fn_name=SimilarityFunction.DOT_PRODUCT)\n",
    "    elif sim_function == \"euclidean\":\n",
    "        model = SentenceTransformer(bert_model, similarity_fn_name=SimilarityFunction.EUCLIDEAN)\n",
    "    elif sim_function == \"manhattan\":\n",
    "        model = SentenceTransformer(bert_model, similarity_fn_name=SimilarityFunction.MANHATTAN)\n",
    "    else:\n",
    "        raise ValueError(\"Wrong 'sim_function' parameter. Only 'cosine', 'dot', 'euclidean', or 'manhattan' is allowed.\")\n",
    "\n",
    "    # BATCH ENCODE ALL TEXTS ONCE\n",
    "    print(f\"Encoding {len(test_df)} test instances...\")\n",
    "    test_embeddings = model.encode(test_df['preprocessed'].tolist(), show_progress_bar=True)\n",
    "\n",
    "    print(f\"Encoding training examples for each category...\")\n",
    "    hsal_1_embeddings = model.encode(hsal_1_df['preprocessed'].tolist(), show_progress_bar=True)\n",
    "    hsal_2_embeddings = model.encode(hsal_2_df['preprocessed'].tolist(), show_progress_bar=True)\n",
    "    hsal_3_embeddings = model.encode(hsal_3_df['preprocessed'].tolist(), show_progress_bar=True)\n",
    "    hsal_4_embeddings = model.encode(hsal_4_df['preprocessed'].tolist(), show_progress_bar=True)\n",
    "\n",
    "    # COMPUTE ALL SIMILARITIES IN BATCH\n",
    "    print(\"Computing similarity scores...\")\n",
    "    hsal_1_similarities = model.similarity(test_embeddings, hsal_1_embeddings).cpu().numpy()\n",
    "    hsal_2_similarities = model.similarity(test_embeddings, hsal_2_embeddings).cpu().numpy()\n",
    "    hsal_3_similarities = model.similarity(test_embeddings, hsal_3_embeddings).cpu().numpy()\n",
    "    hsal_4_similarities = model.similarity(test_embeddings, hsal_4_embeddings).cpu().numpy()\n",
    "\n",
    "    # Ensure the output directory exists\n",
    "    os.makedirs(os.path.dirname(output_file) if os.path.dirname(output_file) else '.', exist_ok=True)\n",
    "\n",
    "    print(f\"Generating examples for {len(test_df)} test instances...\")\n",
    "\n",
    "    # Process each test instance\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        for idx, row in test_df.iterrows():\n",
    "            # Get similarity scores for this instance\n",
    "            hsal_1_sims = hsal_1_similarities[idx]\n",
    "            hsal_2_sims = hsal_2_similarities[idx]\n",
    "            hsal_3_sims = hsal_3_similarities[idx]\n",
    "            hsal_4_sims = hsal_4_similarities[idx]\n",
    "\n",
    "            # Get top N indices based on similarity\n",
    "            # For cosine/dot: higher is better, for euclidean/manhattan: lower is better\n",
    "            if sim_function in [\"cosine\", \"dot\"]:\n",
    "                top_hsal_1_indices = np.argsort(hsal_1_sims)[-num_of_example:][::-1]\n",
    "                top_hsal_2_indices = np.argsort(hsal_2_sims)[-num_of_example:][::-1]\n",
    "                top_hsal_3_indices = np.argsort(hsal_3_sims)[-num_of_example:][::-1]\n",
    "                top_hsal_4_indices = np.argsort(hsal_4_sims)[-num_of_example:][::-1]\n",
    "            else:  # euclidean, manhattan\n",
    "                top_hsal_1_indices = np.argsort(hsal_1_sims)[:num_of_example]\n",
    "                top_hsal_2_indices = np.argsort(hsal_2_sims)[:num_of_example]\n",
    "                top_hsal_3_indices = np.argsort(hsal_3_sims)[:num_of_example]\n",
    "                top_hsal_4_indices = np.argsort(hsal_4_sims)[:num_of_example]\n",
    "\n",
    "            # Extract examples with proper label mapping\n",
    "            bert_ex_hsal = [\n",
    "                (hsal_1_df['preprocessed'].iloc[i], \n",
    "                 label_mapping_hsal[(hsal_1_df['final_label_hs'].iloc[i], hsal_1_df['final_label_al'].iloc[i])])\n",
    "                for i in top_hsal_1_indices\n",
    "            ]\n",
    "\n",
    "            bert_ex_hsnal = [\n",
    "                (hsal_2_df['preprocessed'].iloc[i], \n",
    "                 label_mapping_hsal[(hsal_2_df['final_label_hs'].iloc[i], hsal_2_df['final_label_al'].iloc[i])])\n",
    "                for i in top_hsal_2_indices\n",
    "            ]\n",
    "\n",
    "            bert_ex_nhsal = [\n",
    "                (hsal_3_df['preprocessed'].iloc[i], \n",
    "                 label_mapping_hsal[(hsal_3_df['final_label_hs'].iloc[i], hsal_3_df['final_label_al'].iloc[i])])\n",
    "                for i in top_hsal_3_indices\n",
    "            ]\n",
    "\n",
    "            bert_ex_nhsnal = [\n",
    "                (hsal_4_df['preprocessed'].iloc[i], \n",
    "                 label_mapping_hsal[(hsal_4_df['final_label_hs'].iloc[i], hsal_4_df['final_label_al'].iloc[i])])\n",
    "                for i in top_hsal_4_indices\n",
    "            ]\n",
    "\n",
    "            # Create output dictionary\n",
    "            output = {\n",
    "                \"idx\": int(idx),\n",
    "                \"text\": row['preprocessed'],\n",
    "                \"label\": label_mapping_hsal[(row['final_label_hs'], row['final_label_al'])],\n",
    "                \"bert_ex_hsal\": bert_ex_hsal,\n",
    "                \"bert_ex_hsnal\": bert_ex_hsnal,\n",
    "                \"bert_ex_nhsal\": bert_ex_nhsal,\n",
    "                \"bert_ex_nhsnal\": bert_ex_nhsnal\n",
    "            }\n",
    "\n",
    "            f.write(json.dumps(output, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "            # Progress indicator\n",
    "            if (idx + 1) % 100 == 0:\n",
    "                print(f\"Processed {idx + 1}/{len(test_df)} instances\")\n",
    "\n",
    "    print(f\"✓ {output_file} has been created successfully with BERT-based examples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-24T03:25:59.506208Z",
     "iopub.status.busy": "2025-09-24T03:25:59.505988Z",
     "iopub.status.idle": "2025-09-24T03:28:51.402355Z",
     "shell.execute_reply": "2025-09-24T03:28:51.401613Z",
     "shell.execute_reply.started": "2025-09-24T03:25:59.506193Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT model: indobenchmark/indobert-large-p2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name indobenchmark/indobert-large-p2. Creating a new one with mean pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding 4634 test instances...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 145/145 [00:08<00:00, 17.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding training examples for each category...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 144/144 [00:08<00:00, 16.56it/s]\n",
      "Batches: 100%|██████████| 57/57 [00:03<00:00, 18.95it/s]\n",
      "Batches: 100%|██████████| 164/164 [00:07<00:00, 20.88it/s]\n",
      "Batches: 100%|██████████| 202/202 [00:13<00:00, 15.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing similarity scores...\n",
      "Generating examples for 4634 test instances...\n",
      "Processed 100/4634 instances\n",
      "Processed 200/4634 instances\n",
      "Processed 300/4634 instances\n",
      "Processed 400/4634 instances\n",
      "Processed 500/4634 instances\n",
      "Processed 600/4634 instances\n",
      "Processed 700/4634 instances\n",
      "Processed 800/4634 instances\n",
      "Processed 900/4634 instances\n",
      "Processed 1000/4634 instances\n",
      "Processed 1100/4634 instances\n",
      "Processed 1200/4634 instances\n",
      "Processed 1300/4634 instances\n",
      "Processed 1400/4634 instances\n",
      "Processed 1500/4634 instances\n",
      "Processed 1600/4634 instances\n",
      "Processed 1700/4634 instances\n",
      "Processed 1800/4634 instances\n",
      "Processed 1900/4634 instances\n",
      "Processed 2000/4634 instances\n",
      "Processed 2100/4634 instances\n",
      "Processed 2200/4634 instances\n",
      "Processed 2300/4634 instances\n",
      "Processed 2400/4634 instances\n",
      "Processed 2500/4634 instances\n",
      "Processed 2600/4634 instances\n",
      "Processed 2700/4634 instances\n",
      "Processed 2800/4634 instances\n",
      "Processed 2900/4634 instances\n",
      "Processed 3000/4634 instances\n",
      "Processed 3100/4634 instances\n",
      "Processed 3200/4634 instances\n",
      "Processed 3300/4634 instances\n",
      "Processed 3400/4634 instances\n",
      "Processed 3500/4634 instances\n",
      "Processed 3600/4634 instances\n",
      "Processed 3700/4634 instances\n",
      "Processed 3800/4634 instances\n",
      "Processed 3900/4634 instances\n",
      "Processed 4000/4634 instances\n",
      "Processed 4100/4634 instances\n",
      "Processed 4200/4634 instances\n",
      "Processed 4300/4634 instances\n",
      "Processed 4400/4634 instances\n",
      "Processed 4500/4634 instances\n",
      "Processed 4600/4634 instances\n",
      "✓ /home/alvaro_luqman/resource/Inference Results/HsAl/SahabatAI/test_with_example_bert.jsonl has been created successfully with BERT-based examples.\n"
     ]
    }
   ],
   "source": [
    "get_example_bert_multilabel(\n",
    "    test_file=\"/home/alvaro_luqman/resource/Datasets/HsAl/preprocessed_test.csv\",\n",
    "    train_file=\"/home/alvaro_luqman/resource/Datasets/HsAl/preprocessed_train.csv\",\n",
    "    num_of_example=5,\n",
    "    output_file=\"/home/alvaro_luqman/resource/Inference Results/HsAl/SahabatAI/test_with_example_bert.jsonl\",\n",
    "    bert_model=\"indobenchmark/indobert-large-p2\",  # Good for Indonesian\n",
    "    sim_function=\"cosine\",  # or try \"manhattan\" based on your findings!\n",
    "    drop_duplicate_text=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **EXAMPLE_GENERATOR.PY**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-24T03:28:51.429542Z",
     "iopub.status.busy": "2025-09-24T03:28:51.428886Z",
     "iopub.status.idle": "2025-09-24T03:28:51.446298Z",
     "shell.execute_reply": "2025-09-24T03:28:51.445608Z",
     "shell.execute_reply.started": "2025-09-24T03:28:51.429525Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def example_generator_hsal(ex_hsal, ex_hs, ex_al, ex_non_hsal, prompt_variant):\n",
    "    example = ''\n",
    "    str_prompt = str(prompt_variant)\n",
    "    num_ex = int(str_prompt[2])\n",
    "\n",
    "    # Ambil jumlah contoh yang diminta dari setiap kategori\n",
    "    list_hsal = ex_hsal[:num_ex]\n",
    "    list_hs = ex_hs[:num_ex]\n",
    "    list_al = ex_al[:num_ex]\n",
    "    list_non_hsal = ex_non_hsal[:num_ex]\n",
    "\n",
    "    # Gabungkan secara bergiliran satu per kategori\n",
    "    list_ex = []\n",
    "    for i in range(num_ex):\n",
    "        if i < len(list_hsal):\n",
    "            list_ex.append(list_hsal[i])\n",
    "        if i < len(list_hs):\n",
    "            list_ex.append(list_hs[i])\n",
    "        if i < len(list_al):\n",
    "            list_ex.append(list_al[i])\n",
    "        if i < len(list_non_hsal):\n",
    "            list_ex.append(list_non_hsal[i])\n",
    "\n",
    "    # Format hasil akhir\n",
    "    for i, (teks, label_kode) in enumerate(list_ex):\n",
    "        if label_kode == 'HsAl':\n",
    "            label = 'ujaran_kebencian_kasar'\n",
    "        elif label_kode == 'HsnAl':\n",
    "            label = 'ujaran_kebencian'\n",
    "        elif label_kode == 'nHsAl':\n",
    "            label = 'ujaran_kasar'\n",
    "        else:\n",
    "            label = 'bukan_ujaran_kebencian_kasar'\n",
    "        example += f\"{i+1}. Teks: '{teks}'. Jawaban: '{label}'\\n\"\n",
    "\n",
    "    return example\n",
    "\n",
    "def example_generator_hs(ex_hs, ex_non_hs, prompt_variant):\n",
    "    example = ''\n",
    "    str_prompt = str(prompt_variant)\n",
    "    num_ex = int(str_prompt[2])*2\n",
    "\n",
    "    list_hs = ex_hs[:num_ex]\n",
    "    list_non_hs = ex_non_hs[:num_ex]\n",
    "\n",
    "    # Gabungkan secara bergiliran\n",
    "    list_ex = []\n",
    "    for i in range(num_ex):\n",
    "        if i < len(list_hs):\n",
    "            list_ex.append(list_hs[i])\n",
    "        if i < len(list_non_hs):\n",
    "            list_ex.append(list_non_hs[i])\n",
    "    \n",
    "    # Format hasil akhir\n",
    "    for i, (teks, label_kode) in enumerate(list_ex):\n",
    "        if label_kode == 'HS':\n",
    "            label = 'ujaran_kebencian'\n",
    "        else:\n",
    "            label = 'bukan_ujaran_kebencian'\n",
    "        example += f\"{i+1}. Teks: '{teks}'. Jawaban: '{label}'\\n\"\n",
    "    \n",
    "    return example\n",
    "\n",
    "def example_generator_al(ex_al, ex_non_al, prompt_variant):\n",
    "    example = ''\n",
    "    str_prompt = str(prompt_variant)\n",
    "    num_ex = int(str_prompt[2])*2\n",
    "\n",
    "    list_al = ex_al[:num_ex]\n",
    "    list_non_al = ex_non_al[:num_ex]\n",
    "    \n",
    "    # Gabungkan secara bergiliran\n",
    "    list_ex = []\n",
    "    for i in range(num_ex):\n",
    "        if i < len(list_al):\n",
    "            list_ex.append(list_al[i])\n",
    "        if i < len(list_non_al):\n",
    "            list_ex.append(list_non_al[i])\n",
    "    \n",
    "    # Format hasil akhir\n",
    "    for i, (teks, label_kode) in enumerate(list_ex):\n",
    "        if label_kode == 'Al':\n",
    "            label = 'ujaran_kasar'\n",
    "        else:\n",
    "            label = 'bukan_ujaran_kasar'\n",
    "        example += f\"{i+1}. Teks: '{teks}'. Jawaban: '{label}'\\n\"\n",
    "    \n",
    "    return example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qDu57RPxmoHB"
   },
   "source": [
    "# **PROMPT_APPROACH_1_ZERO.PY**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-24T03:28:51.448659Z",
     "iopub.status.busy": "2025-09-24T03:28:51.448466Z",
     "iopub.status.idle": "2025-09-24T03:28:51.461035Z",
     "shell.execute_reply": "2025-09-24T03:28:51.460418Z",
     "shell.execute_reply.started": "2025-09-24T03:28:51.448645Z"
    },
    "id": "tOKcU7GznSfv",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def prompt_approach_1_zero_hs (list_inference_input, prompt_variant):\n",
    "    if str(prompt_variant).startswith('1'):\n",
    "        return f\"\"\"\n",
    "Tugas: Diberikan sebuah teks pada input berikut, tentukan apakah teks tersebut mengandung ujaran kebencian atau tidak.\n",
    "\n",
    "Instruksi:\n",
    "- Keluaran label jawaban hanya dapat berupa ‘ujaran_kebencian’ untuk teks yang mengandung ujaran kebencian, atau ‘bukan_ujaran_kebencian’ untuk teks yang tidak mengandung ujaran kebencian.\n",
    "- Mohon untuk tidak memberikan penjelasan atas jawaban Anda.\n",
    "\n",
    "Giliran Anda:\n",
    "Input: \n",
    "- Teks: '{list_inference_input[1]}' \n",
    "Jawaban:\n",
    "\"\"\"\n",
    "\n",
    "def prompt_approach_1_zero_al (list_inference_input, prompt_variant):\n",
    "    if str(prompt_variant).startswith('1'):\n",
    "        return f\"\"\"\n",
    "Tugas: Diberikan sebuah teks pada input berikut, tentukan apakah teks tersebut mengandung ujaran kasar atau tidak\n",
    "Instruksi:\n",
    "- Keluaran label jawaban hanya dapat berupa ‘ujaran_kasar’ untuk teks yang mengandung ujaran kasar, atau ‘bukan_ujaran_kasar’ untuk teks yang tidak mengandung ujaran kasar.\n",
    "- Mohon untuk tidak memberikan penjelasan atas jawaban Anda.\n",
    "\n",
    "Giliran Anda:\n",
    "Input: \n",
    "- Teks: '{list_inference_input[1]}' \n",
    "Jawaban:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1wqxM9p_TN8B"
   },
   "source": [
    "# **PROMPT_APPROACH_2_ZERO.PY**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-24T03:28:51.462221Z",
     "iopub.status.busy": "2025-09-24T03:28:51.461975Z",
     "iopub.status.idle": "2025-09-24T03:28:51.477464Z",
     "shell.execute_reply": "2025-09-24T03:28:51.476798Z",
     "shell.execute_reply.started": "2025-09-24T03:28:51.462200Z"
    },
    "id": "t1Op9ckmTN8C",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def prompt_approach_2_zero (list_inference_input, prompt_variant):\n",
    "    if str(prompt_variant).startswith('1'):\n",
    "        return f\"\"\"\n",
    "Tugas: Diberikan sebuah teks pada input berikut, tentukan apakah teks tersebut mengandung ujaran kebencian atau tidak, dan mengandung ujaran kasar atau tidak.\n",
    "\n",
    "Instruksi:\n",
    "- Keluaran label jawaban hanya dapat berupa ‘ujaran_kebencian’ untuk teks yang mengandung ujaran kebencian namun tidak mengandung ujaran kasar, ‘ujaran_kasar’ untuk teks yang mengandung ujaran kasar namun tidak mengandung ujaran kebencian, ‘ujaran_kebencian_kasar’ untuk teks yang mengandung ujaran kebencian dan ujaran kasar, atau ‘bukan_ujaran_kebencian_kasar’ untuk teks yang tidak mengandung ujaran kebencian maupun ujaran kasar.\n",
    "- Mohon untuk tidak memberikan penjelasan atas jawaban Anda.\n",
    "\n",
    "Giliran Anda:\n",
    "Input:\n",
    "- Teks: '{list_inference_input[1]}'\n",
    "Jawaban:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **PROMPT_APPROACH_1_FEW.PY**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-24T03:28:51.478409Z",
     "iopub.status.busy": "2025-09-24T03:28:51.478157Z",
     "iopub.status.idle": "2025-09-24T03:28:51.495701Z",
     "shell.execute_reply": "2025-09-24T03:28:51.495170Z",
     "shell.execute_reply.started": "2025-09-24T03:28:51.478395Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def prompt_approach_1_few_hs(list_inference_input,prompt_variant):\n",
    "  formatted_example = example_generator_hs(list_inference_input[3],list_inference_input[4],prompt_variant)\n",
    "  if str(prompt_variant).startswith('1'):\n",
    "    return f\"\"\"\n",
    "Tugas: Diberikan sebuah teks pada input berikut, tentukan apakah teks tersebut mengandung ujaran kebencian atau tidak. Anda akan diberikan beberapa contoh.\n",
    "\n",
    "Instruksi:\n",
    "- Keluaran label jawaban hanya dapat berupa ‘ujaran_kebencian’ untuk teks yang mengandung ujaran kebencian, atau ‘bukan_ujaran_kebencian’ untuk teks yang tidak mengandung ujaran kebencian.\n",
    "- Contoh yang diberikan dapat membantu Anda dalam menentukan jawaban.\n",
    "- Mohon untuk tidak memberikan penjelasan atas jawaban Anda.\n",
    "\n",
    "Berikut contohnya:\n",
    "{formatted_example}\n",
    "Giliran Anda:\n",
    "Input:\n",
    "- Teks: '{list_inference_input[1]}'\n",
    "Jawaban:\n",
    "\"\"\" \n",
    "\n",
    "def prompt_approach_1_few_al(list_inference_input,prompt_variant):\n",
    "  formatted_example = example_generator_al(list_inference_input[3],list_inference_input[4],prompt_variant)\n",
    "  if str(prompt_variant).startswith('1'):\n",
    "    return f\"\"\"\n",
    "Tugas: Diberikan sebuah teks pada input berikut, tentukan apakah teks tersebut mengandung ujaran kasar atau tidak. Anda akan diberikan beberapa contoh.\n",
    "\n",
    "Instruksi:\n",
    "- Keluaran label jawaban hanya dapat berupa ‘ujaran_kasar’ untuk teks yang mengandung ujaran kasar, atau ‘bukan_ujaran_kasar’ untuk teks yang tidak mengandung ujaran kasar.\n",
    "- Contoh yang diberikan dapat membantu Anda dalam menentukan jawaban.\n",
    "- Mohon untuk tidak memberikan penjelasan atas jawaban Anda.\n",
    "\n",
    "Berikut contohnya:\n",
    "{formatted_example}\n",
    "Giliran Anda:\n",
    "Input:\n",
    "- Teks: '{list_inference_input[1]}'\n",
    "Jawaban:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **PROMPT_APPROACH_2_FEW.PY**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-24T03:28:51.496565Z",
     "iopub.status.busy": "2025-09-24T03:28:51.496381Z",
     "iopub.status.idle": "2025-09-24T03:28:51.513466Z",
     "shell.execute_reply": "2025-09-24T03:28:51.512748Z",
     "shell.execute_reply.started": "2025-09-24T03:28:51.496552Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def prompt_approach_2_few(list_inference_input, prompt_variant):\n",
    "    formatted_example = example_generator_hsal(\n",
    "        list_inference_input[3],\n",
    "        list_inference_input[4],\n",
    "        list_inference_input[5],\n",
    "        list_inference_input[6],\n",
    "        prompt_variant\n",
    "    )\n",
    "    \n",
    "    if str(prompt_variant).startswith('1'):\n",
    "        return f\"\"\"\n",
    "Tugas: Diberikan sebuah teks pada input berikut, tentukan apakah teks tersebut mengandung ujaran kebencian atau tidak, dan mengandung ujaran kasar atau tidak.\n",
    "\n",
    "Instruksi:\n",
    "- Keluaran label jawaban hanya dapat berupa 'ujaran_kebencian' untuk teks yang mengandung ujaran kebencian namun tidak mengandung ujaran kasar, 'ujaran_kasar' untuk teks yang mengandung ujaran kasar namun tidak mengandung ujaran kebencian, 'ujaran_kebencian_kasar' untuk teks yang mengandung ujaran kebencian dan ujaran kasar, atau 'bukan_ujaran_kebencian_kasar' untuk teks yang tidak mengandung ujaran kebencian maupun ujaran kasar.\n",
    "- Contoh yang diberikan dapat membantu Anda dalam menentukan jawaban.\n",
    "- Mohon untuk tidak memberikan penjelasan atas jawaban Anda.\n",
    "\n",
    "Berikut contohnya:\n",
    "{formatted_example}\n",
    "Giliran Anda:\n",
    "Input:\n",
    "- Teks: '{list_inference_input[1]}'\n",
    "Jawaban:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rsWp0jY5nwj1"
   },
   "source": [
    "# **GET_PROMPT.PY**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-24T03:28:51.514396Z",
     "iopub.status.busy": "2025-09-24T03:28:51.514142Z",
     "iopub.status.idle": "2025-09-24T03:28:51.531755Z",
     "shell.execute_reply": "2025-09-24T03:28:51.531157Z",
     "shell.execute_reply.started": "2025-09-24T03:28:51.514376Z"
    },
    "id": "1uDT8SOzn86V",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_prompt(list_inference_input,prompt_approach_type,prompt_variant):\n",
    "  # list_inference_input : list, prompt_approach_type : string, prompt_variant : integer)\n",
    "  if prompt_approach_type == \"approach_1_zero_hs\":\n",
    "    return prompt_approach_1_zero_hs(list_inference_input, prompt_variant)\n",
    "  if prompt_approach_type == \"approach_1_few_hs\":\n",
    "    return prompt_approach_1_few_hs(list_inference_input,prompt_variant)\n",
    "  if prompt_approach_type == \"approach_1_zero_al\":\n",
    "    return prompt_approach_1_zero_al(list_inference_input, prompt_variant)\n",
    "  if prompt_approach_type == \"approach_1_few_al\":\n",
    "    return prompt_approach_1_few_al(list_inference_input,prompt_variant)\n",
    "  if prompt_approach_type == \"approach_2_zero\":\n",
    "    return prompt_approach_2_zero(list_inference_input, prompt_variant)\n",
    "  if prompt_approach_type == \"approach_2_few\":\n",
    "    return prompt_approach_2_few(list_inference_input,prompt_variant)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1DlP09VFo4ZF"
   },
   "source": [
    "# **main.py**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-24T03:28:51.532548Z",
     "iopub.status.busy": "2025-09-24T03:28:51.532373Z",
     "iopub.status.idle": "2025-09-24T03:28:51.548395Z",
     "shell.execute_reply": "2025-09-24T03:28:51.547744Z",
     "shell.execute_reply.started": "2025-09-24T03:28:51.532535Z"
    },
    "id": "bBgkWA0NpHt7",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def get_jsonl_keys(jsonl_file):\n",
    "    with open(jsonl_file, \"r\") as f:\n",
    "        first_line = json.loads(f.readline())  # Membaca satu objek JSON pertama\n",
    "        keys = list(first_line.keys())  # Mendapatkan semua atribut\n",
    "        return keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-24T03:28:51.549194Z",
     "iopub.status.busy": "2025-09-24T03:28:51.548983Z",
     "iopub.status.idle": "2025-09-24T03:28:51.565855Z",
     "shell.execute_reply": "2025-09-24T03:28:51.565277Z",
     "shell.execute_reply.started": "2025-09-24T03:28:51.549178Z"
    },
    "id": "IcJc8gVmoQTn",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import random\n",
    "from sklearn.utils import shuffle\n",
    "import os\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import numpy as np\n",
    "from huggingface_hub import login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-24T03:28:51.566828Z",
     "iopub.status.busy": "2025-09-24T03:28:51.566578Z",
     "iopub.status.idle": "2025-09-24T03:28:51.585418Z",
     "shell.execute_reply": "2025-09-24T03:28:51.584815Z",
     "shell.execute_reply.started": "2025-09-24T03:28:51.566812Z"
    },
    "id": "RUTqcGFKqNdr",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_jsonl_keys(jsonl_file):\n",
    "    with open(jsonl_file, \"r\") as f:\n",
    "        first_line = json.loads(f.readline())  # Membaca satu objek JSON pertama\n",
    "        keys = list(first_line.keys())  # Mendapatkan semua atribut\n",
    "        return keys\n",
    "\n",
    "def load_model_tokenizer(model_name, hf_token=\"\"):\n",
    "    if hf_token != \"\":\n",
    "        login(token=hf_token)\n",
    "        flag_auth_token = True\n",
    "    else:\n",
    "        flag_auth_token = False\n",
    "    \n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_name, \n",
    "        cache_dir = \"/home/shared/huggingface\",\n",
    "        token=hf_token if flag_auth_token else None  # Using token instead of use_auth_token\n",
    "    )\n",
    "    \n",
    "    # Load model with proper configuration for multi-GPU\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        cache_dir = \"/home/shared/huggingface\",\n",
    "        device_map=\"auto\",  # Automatically distribute across GPUs\n",
    "        token=hf_token if flag_auth_token else None,  # Using token instead of use_auth_token\n",
    "        torch_dtype=torch.bfloat16,  # as recommended by the documentation of SahabatAi\n",
    "        # torch_dtype=\"auto\",\n",
    "        # low_cpu_mem_usage=True,      # Reduce CPU memory usage\n",
    "    )\n",
    "    \n",
    "    \n",
    "    # Set pad token if needed\n",
    "    if tokenizer.pad_token_id is None:\n",
    "        tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "        \n",
    "    return model, tokenizer\n",
    "    \n",
    "def llm_inference_greedy_search(prompt,tokenizer,model,gpu_device=\"\",max_new_tokens=250,return_mode=\"with_subtoken_score\"):\n",
    "    if hasattr(model, \"hf_device_map\"):\n",
    "        # The model is already distributed, so we'll just use the current device mapping\n",
    "        device = next(iter(model.parameters())).device  # Get device of first parameter for input tensor\n",
    "        inputs = tokenizer([prompt], return_tensors=\"pt\").to(device)\n",
    "    else:\n",
    "        # Handle single device case\n",
    "        if gpu_device is None or gpu_device == \"\":\n",
    "            device = \"cpu\"\n",
    "        elif isinstance(gpu_device, int):\n",
    "            device = f\"cuda:{gpu_device}\"\n",
    "        elif gpu_device.lower() == \"cuda\":\n",
    "            device = \"cuda\"\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid gpu_device: {gpu_device}\")\n",
    "        \n",
    "        model = model.to(device)\n",
    "        inputs = tokenizer([prompt], return_tensors=\"pt\").to(device)\n",
    "    outputs = model.generate(**inputs, max_new_tokens=max_new_tokens, return_dict_in_generate=True, output_scores=True)\n",
    "    transition_scores = model.compute_transition_scores(\n",
    "        outputs.sequences, outputs.scores, normalize_logits=True\n",
    "    )\n",
    "    input_length = inputs.input_ids.shape[1]\n",
    "    generated_tokens = outputs.sequences[:, input_length:]\n",
    "    original_answer = tokenizer.batch_decode(generated_tokens)[0]\n",
    "    if return_mode == \"without_subtoken_score\":\n",
    "        return original_answer\n",
    "    elif return_mode == \"with_subtoken_score\":\n",
    "        list_subtoken,list_subtoken_score = [],[]\n",
    "        for tok, score in zip(generated_tokens[0], transition_scores[0]):\n",
    "            list_subtoken.append(tokenizer.decode(tok))\n",
    "            if device == \"cpu\":\n",
    "                list_subtoken_score.append(float(np.exp(score.numpy())))\n",
    "            else:\n",
    "                list_subtoken_score.append(float(np.exp(score.cpu().numpy())))\n",
    "        return original_answer, list_subtoken, list_subtoken_score\n",
    "    else:\n",
    "        raise ValueError(\"Wrong `return_mode`. Please type `with_subtoken_score` if you want to get the sub token score, or `without_subtoken_score` if not.\")\n",
    "\n",
    "def write_jsonl(jsons, output_filename):\n",
    "  with open(output_filename, \"w\") as f:\n",
    "    for each_json in jsons:\n",
    "      json.dump(each_json,f)\n",
    "      f.write(\"\\n\")\n",
    "\n",
    "def read_jsonl(filename):\n",
    "  result = []\n",
    "  with open(filename, \"r\") as f:\n",
    "    for line in f.readlines():\n",
    "      result.append(json.loads(line))\n",
    "  return result\n",
    "\n",
    "def llm_inference_bulk(input_file_path,list_inference_attribute,prompt_task_type,prompt_variant,model_name,gpu_device=\"\",hf_token=\"\",max_new_tokens=250,return_mode=\"with_subtoken_score\",verbose=\"yes\"):\n",
    "    # Load model and tokenizer\n",
    "    if verbose == \"yes\":\n",
    "        print(f\"Loading model and tokenizer from pretrained model: {model_name}\")\n",
    "    model,tokenizer = load_model_tokenizer(model_name,hf_token)\n",
    "    # Read jsonl data\n",
    "    jsonl_data = read_jsonl(input_file_path)\n",
    "    if verbose == \"yes\":\n",
    "        len_jsonl_data = len(jsonl_data)\n",
    "    # Loop the inference process for all data in the jsonl file\n",
    "    start_time = time.time()  # Record the start time\n",
    "    max_runtime = 11.5 * 60 * 60 \n",
    "    for js_idx in range(0, len(jsonl_data)):\n",
    "        loop_start_time = time.time()\n",
    "        if verbose == \"yes\":\n",
    "            print(f\"Processing file {js_idx+1} of {len_jsonl_data} total texts.\")\n",
    "        # Get list of inference input to generate the prompt\n",
    "        list_inference_input = []\n",
    "        for attribute in list_inference_attribute:\n",
    "            list_inference_input.append(jsonl_data[js_idx].get(attribute))\n",
    "        prompt = get_prompt(list_inference_input,prompt_task_type,prompt_variant)\n",
    "        print(prompt)\n",
    "        try:\n",
    "            if return_mode == \"without_subtoken_score\":\n",
    "                original_answer = llm_inference_greedy_search(prompt,tokenizer,model,max_new_tokens,return_mode)\n",
    "                jsonl_data[js_idx][\"original_answer\"] = original_answer\n",
    "                print(original_answer)\n",
    "            elif return_mode == \"with_subtoken_score\":\n",
    "                original_answer, list_subtoken, list_subtoken_score = llm_inference_greedy_search(prompt,tokenizer,model,gpu_device,max_new_tokens,return_mode)\n",
    "                jsonl_data[js_idx][\"original_answer\"] = original_answer\n",
    "                jsonl_data[js_idx][\"list_subtoken\"] = list_subtoken\n",
    "                jsonl_data[js_idx][\"list_subtoken_score\"] = list_subtoken_score\n",
    "                print(original_answer)\n",
    "            else:\n",
    "                raise ValueError(\"Wrong `return_mode`. Please type `with_subtoken_score` if you want to get the sub token score, or `without_subtoken_score` if not.\")\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            jsonl_data[js_idx][\"original_answer\"] = \"failed_to_get_inference_result\"\n",
    "            print(f\"Failed to get inference result due to Out of Memory (OOM) on json_idx line: {js_idx+1}. Please check your input length. You may need change llm architecture or prune your input.\")\n",
    "        loop_end_time = time.time()  # End timer for the loop\n",
    "        loop_duration = loop_end_time - loop_start_time\n",
    "        print(f\"Processing time for file {js_idx+1}: {loop_duration:.2f} seconds\")\n",
    "    print(\"Inference bulk process is done.\")\n",
    "    return jsonl_data\n",
    "\n",
    "def llm_inference_bulk_file2file(input_file_path,output_file_path,list_inference_attribute,prompt_task_type,prompt_variant,model_name,gpu_device=\"\",hf_token=\"\",max_new_tokens=250,return_mode=\"with_subtoken_score\",verbose=\"yes\"):\n",
    "    inference_result = llm_inference_bulk(input_file_path,list_inference_attribute,prompt_task_type,prompt_variant,model_name,gpu_device,hf_token,max_new_tokens,return_mode,verbose)\n",
    "    # if verbose ==\"yes\":\n",
    "    print(\"Process to save inference result into desired destination.\")\n",
    "    write_jsonl(inference_result,output_file_path)\n",
    "    # if verbose == \"yes\":\n",
    "    print(f\"The inference result has been saved into: {output_file_path}\")\n",
    "\n",
    "def main():\n",
    "    # Get all arguments\n",
    "    # input_file_path = args.input_file_path\n",
    "    # output_file_path = args.output_file_path\n",
    "    # list_inference_attribute = args.list_inference_attribute\n",
    "    # list_inference_attribute = list_inference_attribute.split(\",\")\n",
    "    # prompt_task_type = args.prompt_task_type\n",
    "    # prompt_variant = args.prompt_variant\n",
    "    # model_name = args.model_name\n",
    "    # gpu_device = args.gpu_device\n",
    "    # if gpu_device == \"\":\n",
    "    #     print(\"We will run inference process on CPU device.\")\n",
    "    # else:\n",
    "    #     print(f\"We will run inference process on GPU: {gpu_device} device.\")\n",
    "    # hf_token = args.hf_token\n",
    "    # max_new_tokens = args.max_new_tokens\n",
    "    # return_mode = args.return_mode\n",
    "    # verbose = args.verbose\n",
    "    # # Bulk inference process\n",
    "    input_file_path = '/home/alvaro_luqman/resource/Inference Results/HsAl/SahabatAI/test_with_example_bert.jsonl'\n",
    "    output_file_path  = '/home/alvaro_luqman/resource/Inference Results/HsAl/SahabatAI/output_hsal_sahabatai_few_bert_embed.jsonl'\n",
    "    list_inference_attribute = get_jsonl_keys(input_file_path)\n",
    "    prompt_task_type = \"approach_2_few\"\n",
    "    prompt_variant = 111 #115\n",
    "    model_name = 'GoToCompany/llama3-8b-cpt-sahabatai-v1-instruct'\n",
    "    gpu_device = ''\n",
    "    hf_token = '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'\n",
    "    max_new_tokens = 15 # 15\n",
    "    return_mode = 'with_subtoken_score'\n",
    "    verbose = 'yes'\n",
    "    llm_inference_bulk_file2file(input_file_path,output_file_path,list_inference_attribute,prompt_task_type,prompt_variant,model_name,gpu_device,hf_token,max_new_tokens,return_mode,verbose)\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     parser = argparse.ArgumentParser()\n",
    "#     parser.add_argument(\n",
    "#         \"--input_file_path\", help=\"Your .jsonl input file path.\",\n",
    "#         type=str\n",
    "#     )\n",
    "#     parser.add_argument(\n",
    "#         \"--output_file_path\", help=\"Your .jsonl output file path.\",\n",
    "#         type=str\n",
    "#     )\n",
    "#     parser.add_argument(\n",
    "#         \"--list_inference_attribute\", help=\"Your inference attribute list. Please define as string separated by coma (,). Example: text,frame,polarity\",\n",
    "#         type=str, default=\"text\"\n",
    "#     )\n",
    "#     parser.add_argument(\n",
    "#         \"--prompt_task_type\", help=\"Your prompt task type. You can define your own prompt task type in get_prompt.py file.\",\n",
    "#         type=str\n",
    "#     )\n",
    "#     parser.add_argument(\n",
    "#         \"--prompt_variant\", help=\"Your prompt task variant. You can define your own prompt task variant in get_prompt.py file.\",\n",
    "#         type=int, default=1\n",
    "#     )\n",
    "#     parser.add_argument(\n",
    "#         \"--model_name\", help=\"Your model_name path. It can be your own local model or HuggingFace model name.\",\n",
    "#         type=str, default=\"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "#     )\n",
    "#     parser.add_argument(\n",
    "#         \"--gpu_device\", help=\"The maximum number of new token generated by the LLM.\",\n",
    "#         type=str, default=\"\"\n",
    "#     )\n",
    "#     parser.add_argument(\n",
    "#         \"--hf_token\", help=\"Your HuggingFace token (optional only when you use model from gated repository).\",\n",
    "#         type=str, default=\"\"\n",
    "#     )\n",
    "#     parser.add_argument(\n",
    "#         \"--max_new_tokens\", help=\"The maximum number of new token generated by the LLM.\",\n",
    "#         type=int, default=250\n",
    "#     )\n",
    "#     parser.add_argument(\n",
    "#         \"--return_mode\", help=\"Your inference return mode, whether you only want get generated token (choose `without_subtoken_score`) or with the sub token score (choose `with_subtoken_score`).\",\n",
    "#         type=str, default=\"with_subtoken_score\"\n",
    "#     )\n",
    "#     parser.add_argument(\n",
    "#         \"--verbose\", help=\"Option for showing progress. Chose `yes` for showing complete progress for each sentence, chose `no` if you completely do not want to show the progress.\",\n",
    "#         type=str, default=\"yes\"\n",
    "#     )\n",
    "#     args = parser.parse_args()\n",
    "#     main(args)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-09-24T09:08:51.095Z",
     "iopub.execute_input": "2025-09-24T03:28:51.586257Z",
     "iopub.status.busy": "2025-09-24T03:28:51.586044Z"
    },
    "id": "_P8s5bKSoXlm",
    "outputId": "8aed7b61-b083-46b6-9571-134c64888e69",
    "trusted": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'main' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mmain\u001b[49m()\n",
      "\u001b[31mNameError\u001b[39m: name 'main' is not defined"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-09-24T09:08:51.097Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# !nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-09-24T09:08:51.097Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import unicodedata\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Map compact labels to full labels\n",
    "def map_compact_label(label_code):\n",
    "    mapping = {\n",
    "        \"nHsnAl\": \"bukan_ujaran_kebencian_kasar\",\n",
    "        \"HsnAl\": \"ujaran_kebencian\",\n",
    "        \"nHsAl\": \"ujaran_kasar\",\n",
    "        \"HsAl\": \"ujaran_kebencian_kasar\",\n",
    "    }\n",
    "    return mapping.get(label_code, None)\n",
    "\n",
    "# Clean original answer (LLM prediction)\n",
    "# Your labels (unsorted yet)\n",
    "valid_labels = [\n",
    "    \"bukan_ujaran_kebencian_kasar\",\n",
    "    \"ujaran_kebencian_kasar\",\n",
    "    \"ujaran_kebencian\",\n",
    "    \"ujaran_kasar\",\n",
    "]\n",
    "\n",
    "# Sort valid labels by length (longest first)\n",
    "\n",
    "def clean_original_answer(original_answer):\n",
    "    if original_answer is None:\n",
    "        return \"bukan_ujaran_kebencian_kasar\"\n",
    "\n",
    "    # Step 1: Normalize weird unicode\n",
    "    answer = unicodedata.normalize(\"NFKD\", original_answer)\n",
    "    answer = answer.encode(\"ascii\", \"ignore\").decode(\"ascii\")\n",
    "\n",
    "    # Step 2: Remove unwanted symbols\n",
    "    answer = answer.replace(\"'\", \"\").replace('\"', \"\").replace(\"-\", \"\").replace(\"</s>\", \"\")\n",
    "    answer = re.sub(r\"<.*?>\", \"\", answer)\n",
    "    answer = answer.replace(\"`\", \"\")  # Remove backticks too\n",
    "    answer = answer.strip()\n",
    "\n",
    "    # Step 3: Lowercase\n",
    "    answer = answer.lower()\n",
    "\n",
    "    if \"bukan_ujaran_kebencian_k\" in line:\n",
    "        return \"bukan_ujaran_kebencian_kasar\"\n",
    "    if \"ujaran_kebencian_k\" in line:\n",
    "        return \"ujaran_kebencian_kasar\"\n",
    "    if \"ujaran_ka\" in line:\n",
    "        return \"ujaran_kasar\"\n",
    "    if \"ujaran_ke\" in line:\n",
    "        return \"ujaran_kebencian\"\n",
    "\n",
    "    # Step 4: Match exact label\n",
    "    for label in valid_labels:\n",
    "        if label in answer:\n",
    "            return label\n",
    "\n",
    "    # Step 5: Keyword smart matching\n",
    "    if \"bukan ujaran kebencian kasar\" in answer:\n",
    "        return \"bukan_ujaran_kebencian_kasar\"\n",
    "    if \"ujaran kebencian kasar\" in answer:\n",
    "        return \"ujaran_kebencian_kasar\"\n",
    "    if \"ujaran kebencian\" in answer:\n",
    "        return \"ujaran_kebencian\"\n",
    "    if \"ujaran kasar\" in answer:\n",
    "        return \"ujaran_kasar\"\n",
    "    \n",
    "    return \"bukan_ujaran_kebencian_kasar\"\n",
    "\n",
    "def process_answers(original_answer):\n",
    "    if original_answer is None:\n",
    "        return \"bukan_ujaran_kebencian_kasar\"\n",
    "        \n",
    "    if original_answer == \"0\":\n",
    "      return \"ujaran_kebencian_kasar\"\n",
    "    elif original_answer == \"1\":\n",
    "      return \"ujaran_kebencian\"\n",
    "    elif original_answer == \"2\":\n",
    "      return \"ujaran_kasar\"\n",
    "    elif original_answer == \"3\":\n",
    "      return \"bukan_ujaran_kebencian_kasar\"\n",
    "\n",
    "    return \"bukan_ujaran_kebencian_kasar\"\n",
    "\n",
    "# Load cleaned data\n",
    "input_path = \"/home/alvaro_luqman/resource/Inference Results/HsAl/SahabatAI/output_hsal_sahabatai_few_bert_embed.jsonl\"\n",
    "output_path = \"/home/alvaro_luqman/resource/Inference Results/HsAl/SahabatAI/processed_hsal_sahabatai_few_bert.jsonl\"\n",
    "\n",
    "data = []\n",
    "processed_data = []\n",
    "\n",
    "with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        item = json.loads(line)\n",
    "        item[\"original_answer\"] = clean_original_answer(item.get(\"original_answer\", \"\"))\n",
    "        # item[\"original_answer\"] = process_answers(item.get(\"original_answer\", \"\"))\n",
    "        processed_data.append(item)\n",
    "        data.append(item)  # for evaluation use\n",
    "\n",
    "# Save processed data to a new JSONL file (label not changed!)\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as out_f:\n",
    "    for item in processed_data:\n",
    "        json.dump(item, out_f, ensure_ascii=False)\n",
    "        out_f.write(\"\\n\")\n",
    "\n",
    "print(f\"✅ Processed data saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-09-24T09:08:51.097Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Evaluation (label is mapped here)\n",
    "y_true = []\n",
    "y_pred = []\n",
    "unprocessable_items = 0\n",
    "correct_predictions = 0\n",
    "\n",
    "for item in data:\n",
    "    predicted_label = item[\"original_answer\"]\n",
    "    mapped_label = map_compact_label(item.get(\"label\", \"\"))\n",
    "\n",
    "    if predicted_label is None or mapped_label is None:\n",
    "        unprocessable_items += 1\n",
    "        continue\n",
    "\n",
    "    y_true.append(mapped_label)\n",
    "    y_pred.append(predicted_label)\n",
    "\n",
    "    if predicted_label == mapped_label:\n",
    "        correct_predictions += 1\n",
    "\n",
    "# Accuracy calculation\n",
    "total_evaluated = len(y_true)\n",
    "accuracy = correct_predictions / total_evaluated if total_evaluated > 0 else 0\n",
    "\n",
    "print(f\"\\nAccuracy: {accuracy:.4f} ({correct_predictions}/{total_evaluated})\")\n",
    "print(f\"Total samples: {len(data)}\")\n",
    "print(f\"Unprocessable items: {unprocessable_items}\")\n",
    "\n",
    "# Classes\n",
    "classes = sorted(set(y_true + y_pred))\n",
    "print(f\"Classes: {classes}\")\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred, labels=classes)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Classification Report\n",
    "report = classification_report(y_true, y_pred, labels=classes, output_dict=True)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_true, y_pred, labels=classes))\n",
    "\n",
    "# Per-class accuracy\n",
    "for i, class_name in enumerate(classes):\n",
    "    class_total = sum(y == class_name for y in y_true)\n",
    "    class_correct = cm[i, i]\n",
    "    class_accuracy = class_correct / class_total if class_total > 0 else 0\n",
    "    print(f\"{class_name} accuracy: {class_accuracy:.4f} ({class_correct}/{class_total})\")\n",
    "\n",
    "# Additional metrics\n",
    "print(\"\\nOverall metrics:\")\n",
    "print(f\"Accuracy: {report['accuracy']:.4f}\")\n",
    "print(f\"Weighted F1: {report['weighted avg']['f1-score']:.4f}\")\n",
    "print(f\"Macro F1: {report['macro avg']['f1-score']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-09-24T09:08:51.097Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Get classification report as a dictionary\n",
    "report = classification_report(y_true, y_pred, labels=classes, output_dict=True)\n",
    "\n",
    "# Print metrics for each class\n",
    "for label in classes:\n",
    "    print(f\"\\n{label} metrics:\")\n",
    "    print(f\"Precision: {report[label]['precision']:.4f}\")\n",
    "    print(f\"Recall: {report[label]['recall']:.4f}\")\n",
    "    print(f\"F1 Score: {report[label]['f1-score']:.4f}\")\n",
    "\n",
    "# Print overall metrics\n",
    "print(\"\\nOverall metrics:\")\n",
    "print(f\"Accuracy: {report['accuracy']:.4f}\")\n",
    "print(f\"Weighted F1: {report['weighted avg']['f1-score']:.4f}\")\n",
    "print(f\"Macro F1: {report['macro avg']['f1-score']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-09-24T09:08:51.097Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def excel_safe(text):\n",
    "    if text is None:\n",
    "        return \"\"\n",
    "    text = str(text)\n",
    "    if text.startswith(\"=\"):\n",
    "        text = \"'\" + text\n",
    "    # Always quote the field\n",
    "    text = f'\"{text}\"'\n",
    "    return text\n",
    "\n",
    "with open(input_path, 'r', encoding='utf-8') as jsonl_file, \\\n",
    "     open(\"/home/alvaro_luqman/resource/Inference Results/HsAl/SahabatAI/processed_hsal_sahabatai_few_bert.csv\", 'w', newline='', encoding='utf-8') as csv_file:\n",
    "\n",
    "    writer = csv.writer(csv_file, delimiter=';')\n",
    "    writer.writerow(['text', 'label', 'original_answer', 'preprocessed_answer'])  # Header\n",
    "\n",
    "    for line in jsonl_file:\n",
    "        data = json.loads(line)\n",
    "        text = data.get('text', '')\n",
    "        label = data.get('label', '')\n",
    "        original_answer = data.get('original_answer', '')\n",
    "        preprocessed_answer = clean_original_answer(original_answer)\n",
    "\n",
    "        # Cegah Excel menganggap formula\n",
    "        if preprocessed_answer is not None and preprocessed_answer.startswith('='):\n",
    "            preprocessed_answer = \"'\" + preprocessed_answer\n",
    "\n",
    "        writer.writerow([\n",
    "            excel_safe(text),\n",
    "            excel_safe(label),\n",
    "            excel_safe(original_answer),\n",
    "            excel_safe(preprocessed_answer)\n",
    "        ])\n",
    "\n",
    "print(f\"✅ Berhasil mengubah {input_path} menjadi processed_output_sahabatai_few.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-09-24T09:08:51.098Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # Example usage\n",
    "# input_path = \"/kaggle/working/output_qwen_few.jsonl\"  # Directory containing JSONL files or path to a single JSONL file\n",
    "# output_path = \"output_qwen_few_shot.csv\"\n",
    "\n",
    "# process_jsonl_files(input_path, output_path)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7236169,
     "sourceId": 11729385,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8186272,
     "sourceId": 12936712,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8296733,
     "sourceId": 13098048,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "07f429ba42404acdac33add53efd84df": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4d6fa102172d4b64bdc53675879f0d3e",
      "placeholder": "​",
      "style": "IPY_MODEL_339cb7392aec4b08869dd971d3636aab",
      "value": "Loading checkpoint shards:   0%"
     }
    },
    "302948060d204e48bb79e23a7a5692cc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "339cb7392aec4b08869dd971d3636aab": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3c3a669282894863a7e78ec59a7e5d65": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "40ac3c2d34f84f6dbde10dde517d15ae": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4d6fa102172d4b64bdc53675879f0d3e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "877542ec09394732b4db208611dffb58": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "b268a566ebe6496a9fc3027a181bf9b4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e0a8ca4c13dc416f8f6af8ecc16ad6f2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3c3a669282894863a7e78ec59a7e5d65",
      "max": 3,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_877542ec09394732b4db208611dffb58",
      "value": 0
     }
    },
    "e27a4398fef948818dcfc8d106273ea1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_07f429ba42404acdac33add53efd84df",
       "IPY_MODEL_e0a8ca4c13dc416f8f6af8ecc16ad6f2",
       "IPY_MODEL_fdb432270c8b4b1c972fa3972991b98a"
      ],
      "layout": "IPY_MODEL_302948060d204e48bb79e23a7a5692cc"
     }
    },
    "fdb432270c8b4b1c972fa3972991b98a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_40ac3c2d34f84f6dbde10dde517d15ae",
      "placeholder": "​",
      "style": "IPY_MODEL_b268a566ebe6496a9fc3027a181bf9b4",
      "value": " 0/3 [00:00&lt;?, ?it/s]"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
